import requests
from bs4 import BeautifulSoup
import time

# ê¸°ë³¸ URL ì„¤ì •
BASE_URL = "https://berlinstartupjobs.com"
ENGINEERING_URL = f"{BASE_URL}/engineering/"
SKILL_URL_TEMPLATE = f"{BASE_URL}/skill-areas/{{}}/"

# í¬ë¡¤ë§í•  ìŠ¤í‚¬ ëª©ë¡
skills = ["python", "typescript", "javascript", "rust"]

# ìš”ì²­ í—¤ë” ì„¤ì •

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}


# ì±„ìš© ê³µê³  í¬ë¡¤ë§ í•¨ìˆ˜
def scrape_jobs(url):
    jobs = []
    page = 1

    while True:
        response = requests.get(
            url if page == 1 else f"{url}page/{page}/",
            headers=HEADERS,
        )

        # ì™œ 404 ì½”ë“œê°’ì´ ë‚˜ì˜¤ëŠ”ì§€ ëª¨ë¥´ê² ì§€ë§Œ..ì¼ë‹¨ ìŠ¤í‚µ.. ì œì¶œí•˜ê³ ë‚˜ì„œ ë‚˜ì¤‘ì— ìƒê°!
        # if response.status_code != 200:
        # print(f"âš ï¸ Failed to fetch {url} (Status Code: {response.status_code})")
        # break

        soup = BeautifulSoup(response.text, "html.parser")
        job_listings = soup.find_all("li", class_="bjs-jlid")  # ìƒˆë¡œìš´ í´ë˜ìŠ¤ ë°˜ì˜

        if not job_listings:
            break  # ë‹¤ìŒ í˜ì´ì§€ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ

        for job in job_listings:
            title_tag = job.find("h4", class_="bjs-jlid__h")  # ì§ë¬´ ì œëª©
            company_tag = job.find("a", class_="bjs-jlid__b")  # íšŒì‚¬ ì´ë¦„
            desc_tag = job.find("div", class_="bjs-jlid__description")  # ì„¤ëª…
            link_tag = title_tag.find("a") if title_tag else None  # ì§ë¬´ ìƒì„¸ ë§í¬

            job_data = {
                "title": title_tag.text.strip() if title_tag else "N/A",
                "company": company_tag.text.strip() if company_tag else "N/A",
                "description": desc_tag.text.strip() if desc_tag else "N/A",
                "link": (
                    link_tag["href"] if link_tag and "href" in link_tag.attrs else "N/A"
                ),
            }
            print(job_data)
            jobs.append(job_data)

        print(f"âœ… Scraped Page {page}: {len(job_listings)} jobs found.")
        page += 1
        time.sleep(1)  # ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ë”œë ˆì´

    return jobs


# 1ï¸âƒ£ ì—”ì§€ë‹ˆì–´ë§ ì „ì²´ ì±„ìš© í¬ë¡¤ë§
engineering_jobs = scrape_jobs(ENGINEERING_URL)
print(f"ğŸ”¹ Total Engineering Jobs Scraped: {len(engineering_jobs)}")

# 2ï¸âƒ£ íŠ¹ì • ê¸°ìˆ ë³„ ì±„ìš© í¬ë¡¤ë§
skill_jobs = {}
for skill in skills:
    print(f"ğŸ” Scraping jobs for skill: {skill}")
    skill_jobs[skill] = scrape_jobs(SKILL_URL_TEMPLATE.format(skill))
    print(f"âœ… {skill.upper()} Jobs Scraped: {len(skill_jobs[skill])}")

# ê²°ê³¼ ì¶œë ¥
for skill, jobs in skill_jobs.items():
    print(f"\nğŸ”¹ {skill.upper()} JOBS ({len(jobs)} found)")
    for job in jobs[:3]:  # ì²˜ìŒ 3ê°œ ë¯¸ë¦¬ë³´ê¸°
        print(f"- {job['title']} at {job['company']} ({job['link']})")
